**主要是针对训练过程中的瓶颈进行分析排查，以便于充分压榨GPU性能，提升训练的效率。**

## GPU使用率查询

最简单的就是使用`nvidia-smi -l 1`命令查看GPU使用率，如果GPU使用率为0优先检查代码是否调用了GPU进行计算。如果GPU使用率已经为90%+,此时如果仍然觉得训练国漫，可以考虑多卡训练。

## 瓶颈分析

首先应该确认训练模型的特点：

1. 小模型、数据预处理简单。比如用LeNet训练MNIST，这种情况优化的余地小，因为模型本身对算力的需求小，适合用一般的GPU来训练即可，用越好的GPU使用率会越低。这种场景GPU的使用率特点是保持在一个较低的水平，但是波动小。
2. 小模型、数据预处理较复杂。比如用ResNet18层网络跑ImageNet分类，这种情况CPU预处理会占用更长周期，而GPU的计算非常快占用时长短，因此适合选择更好的CPU和一般的GPU。这种场景GPU的使用率特点是波动大，峰值比较高，然后大部分时间都很低。
3. 大模型、数据预处理简单。这种情况一般GPU都会利用很高且波动小，但是对磁盘的要求也很高，如果利用率低那么请参考下述的方法压榨性能。
4. 大模型，数据预处理复杂。这种情况对CPU和GPU的要求都很高，都可能成为瓶颈，包括磁盘性能，需具体算法具体分析。

对于情况1，优化余地较小。

对于情况2，建议将数据的预处理工作提前进行，不要采用在线预处理的方式。训练时直接读取已经处理好的数据集，可以大大加快训练速度。

对于3和4如果发现GPU利用率较低时可以按下述方法排查瓶颈，优化性能。

### CPU性能

如果GPU占用率为0说明代码可能没有使用GPU，需检查代码。

如果GPU占用率忽高忽低、占用率峰值在50%以下，那么可能是数据预处理跟不上GPU的处理速度。

此时就可以去查看CPU占用率，如果占用率过高，则是说明CPU出现了瓶颈。

如果CPU占用率较低，说明代码没有把CPU的算力压榨出来，一般可以通过修改Torch Dataloader中的worker_num提高CPU负载，经验值num_worker = 略小于核心数量，可以测试不同worker num值对性能的影响，测试出最佳的参数后再开始正式训练。

### 代码层面

1. 每次迭代中做一些与计算无关的操作。比如保存测试图片等等，解决办法是拉长保存测试图片的周期，避免每次迭代都做额外耗时的操作
2. 频繁保存模型，导致保存模型占用了训练过程一定比例的时间，不建议太过频繁的保存模型，因为与磁盘的交互相比于和显存之间的交互实在太慢太慢了。

## others

### pytorch线程数问题

由于默认情况下，PyTorch会创建与核数相同的线程数做相关的计算。同一台服务器中有多卡，经常会同时跑多个人的程序，每个PyTorch进程都会默认创建与核数相同的线程数，导致系统大量时间陷入线程调度而非计算，计算速度大幅减慢，CPU与GPU利用率都很低。可以使用代码`torch.set_num_threads(N)`设置单个进程所开启的线程数。





